{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/train.csv\", dtype={\"project_essay_3\": object, \"project_essay_4\": object})\ntarget = train['project_is_approved']\ntrain = train.drop('project_is_approved', axis=1)\n\ntest = pd.read_csv(\"../input/test.csv\", dtype={\"project_essay_3\": object, \"project_essay_4\": object})\nresources = pd.read_csv(\"../input/resources.csv\")\n\ntrain.fillna(('unk'), inplace=True)\ntest.fillna(('unk'), inplace=True)",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e88cb5cefa4694f3eae3aaad2beccf6b48ce3b11"
      },
      "cell_type": "code",
      "source": "from sklearn import preprocessing\nfrom tqdm import tqdm\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\n\nfeatures = [\n    'teacher_id', \n    'teacher_prefix', \n    'school_state', \n    'project_grade_category',\n    'project_subject_categories', \n    'project_subject_subcategories']\n\ndf_all = pd.concat([train, test], axis=0)\n    \nfor c in tqdm(features):\n    le = LabelEncoder()\n    le.fit(df_all[c].astype(str))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 6/6 [00:01<00:00,  4.46it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "999d2fa04f89334343a4a2470e28ca42614c2951"
      },
      "cell_type": "code",
      "source": "# Feature engineering\n\n# Date and time\ntrain['project_submitted_datetime'] = pd.to_datetime(train['project_submitted_datetime'])\ntest['project_submitted_datetime'] = pd.to_datetime(test['project_submitted_datetime'])\n\n# Date as int may contain some ordinal value\ntrain['datetime_int'] = train['project_submitted_datetime'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['datetime_int'] = test['project_submitted_datetime'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n\n# Date parts\ntrain[\"year\"] = train[\"project_submitted_datetime\"].dt.year\ntrain[\"month\"] = train[\"project_submitted_datetime\"].dt.month\n#train['weekday'] = train['project_submitted_datetime'].dt.weekday\ntrain[\"hour\"] = train[\"project_submitted_datetime\"].dt.hour\ntrain[\"month_Day\"] = train['project_submitted_datetime'].dt.day\n#train[\"year_Day\"] = train['project_submitted_datetime'].dt.dayofyear\ntrain['datetime_dow'] = train['project_submitted_datetime'].dt.dayofweek\ntrain = train.drop('project_submitted_datetime', axis=1)\n\n\n# ****** Test data *********\ntest[\"year\"] = test[\"project_submitted_datetime\"].dt.year\ntest[\"month\"] = test[\"project_submitted_datetime\"].dt.month\n#test['weekday'] = test['project_submitted_datetime'].dt.weekday\ntest[\"hour\"] = test[\"project_submitted_datetime\"].dt.hour\ntest[\"month_Day\"] = test['project_submitted_datetime'].dt.day\n#test[\"year_Day\"] = test['project_submitted_datetime'].dt.dayofyear\ntest['datetime_dow'] = test['project_submitted_datetime'].dt.dayofweek\ntest = test.drop('project_submitted_datetime', axis=1)\n\n# Essay length\ntrain['e1_length'] = train['project_essay_1'].apply(len)\ntest['e1_length'] = train['project_essay_1'].apply(len)\n\ntrain['e2_length'] = train['project_essay_2'].apply(len)\ntest['e2_length'] = train['project_essay_2'].apply(len)\n\n# Title length\ntrain['project_title_len'] = train['project_title'].apply(lambda x: len(str(x)))\ntest['project_title_len'] = test['project_title'].apply(lambda x: len(str(x)))\n\n# Project resource summary length\ntrain['project_resource_summary_len'] = train['project_resource_summary'].apply(lambda x: len(str(x)))\ntest['project_resource_summary_len'] = test['project_resource_summary'].apply(lambda x: len(str(x)))\n\n# Has more than 2 essays?\ntrain['has_gt2_essays'] = train['project_essay_3'].apply(lambda x: 0 if x == 'unk' else 1)\ntest['has_gt2_essays'] = test['project_essay_3'].apply(lambda x: 0 if x == 'unk' else 1)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "317de6d539fae635556f63432ccc8a0541c2b691"
      },
      "cell_type": "code",
      "source": "resources['resources_total'] = resources['quantity'] * resources['price']\n\ndfr = resources.groupby(['id'], as_index=False)[['resources_total']].sum()\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['resources_total']].mean()\ndfr = dfr.rename(columns={'resources_total':'resources_total_mean'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['quantity']].count()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_count'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['quantity']].sum()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_sum'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\n# We're done with IDs for now\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "05e5a728778d2f0d82a8572a398903606bcf8394"
      },
      "cell_type": "code",
      "source": "train['project_essay'] = train.apply(lambda row: ' '.join([\n    str(row['project_title']),\n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']),\n    str(row['project_essay_4']),\n    str(row['project_resource_summary'])]), axis=1)\ntest['project_essay'] = test.apply(lambda row: ' '.join([\n    str(row['project_title']),\n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']),\n    str(row['project_essay_4']),\n    str(row['project_resource_summary'])]), axis=1)\n\ntrain = train.drop([\n    'project_title',\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4',\n    'project_resource_summary'], axis=1)\ntest = test.drop([\n    'project_title',\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4',\n    'project_resource_summary'], axis=1)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "9b27fa3d8c9d27dbc486faf6e418996348dc6f3d"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import punkt\nimport re\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef prep_text(text):\n    q = \"[\\'\\’\\´\\ʻ]\"\n    text = text.strip().lower()\n    text = re.sub('\\W+',' ', text)\n    text = re.sub(r'(\\\")', ' ', text)\n    text = re.sub(r\"\\\\r|\\\\n\", \" \", text)\n    text = re.sub(re.compile(\"won%st\" % q), \"will not\", text)\n    text = re.sub(re.compile(\"can%st\" % q), \"can not\", text)\n    text = re.sub(re.compile(\"n%st\" % q), \" not\", text)\n    text = re.sub(re.compile(\"%sre\" % q), \" are\", text)\n    text = re.sub(re.compile(\"%ss\" % q), \" is\", text)\n    text = re.sub(re.compile(\"%sd\" % q), \" would\", text)\n    text = re.sub(re.compile(\"%sll\" % q), \" will\", text)\n    text = re.sub(re.compile(\"%st\" % q), \" not\", text)\n    text = re.sub(re.compile(\"%sve\" % q), \" have\", text)\n    text = re.sub(re.compile(\"%sm\" % q), \" am\", text)\n    text = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n    return text\n\ntrain['project_essay'] = train['project_essay'].apply(lambda x: prep_text(x))\ntest['project_essay'] = test['project_essay'].apply(lambda x: prep_text(x))",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e3da73f2d41a5a1085bbb85be5080e75ab098507"
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfv = TfidfVectorizer(norm='l2', min_df=0,  max_features=8000, \n            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n            ngram_range=(1,2), use_idf=True, smooth_idf=False, sublinear_tf=True,\n            stop_words = 'english')",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "57dc238961a4154ed8846cf279ce382dcfe18ef6"
      },
      "cell_type": "code",
      "source": "train_text = train['project_essay'].apply(lambda x: ' '.join(x))\ntest_text = test['project_essay'].apply(lambda x: ' '.join(x))\n\n# Fitting tfidf on train + test might be leaky\ntfv.fit(list(train_text.values) + list(test_text.values))\ntrain_tfv = tfv.transform(train_text)\ntest_tfv = tfv.transform(test_text)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27423793e9ee2e016108d8db0439558eb727c77a"
      },
      "cell_type": "code",
      "source": "from scipy.sparse import hstack, csr_matrix\nfeat_train = train.drop('project_essay', axis=1)\nfeat_test = test.drop('project_essay', axis=1)\n\nfeat_train = csr_matrix(feat_train.values)\nfeat_test = csr_matrix(feat_test.values)\n\nX_train_stack = hstack([feat_train, train_tfv[0:feat_train.shape[0]]])\nX_test_stack = hstack([feat_test, test_tfv[0:feat_test.shape[0]]])\n\nprint('Train shape: ', X_train_stack.shape, '\\n\\nTest Shape: ', X_test_stack.shape)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train shape:  (182080, 8022) \n\nTest Shape:  (78035, 8022)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a92c210888b6c02f11bfa77c7764c2d267c5410"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.model_selection import train_test_split\nimport random\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\n\nprint(\"Building model using Light GBM and finding AUC(Area Under Curve)\")\n\ncnt = 0\np_buf = []\nn_splits = 5\nn_repeats = 1\nkf = RepeatedKFold(\n    n_splits=n_splits, \n    n_repeats=n_repeats, \n    random_state=28)\nauc_buf = []  \n\nfor train_index, valid_index in kf.split(X_train_stack):\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train_stack, target, test_size=0.20, random_state=random.seed(28))\n    print('Fold {}/{}'.format(cnt + 1, n_splits))\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 7,\n        'num_leaves': 32,\n        'learning_rate': 0.02,\n        'feature_fraction': 0.80,\n        'bagging_fraction': 0.80,\n        'bagging_freq': 5,\n        'verbose': 0,\n        'lambda_l2': 1,\n    }  \n\n    model = lgb.train(\n        params,\n        lgb.Dataset(X_train, y_train),\n        num_boost_round=10000,\n        valid_sets=[lgb.Dataset(X_valid, y_valid)],\n        early_stopping_rounds=50,\n        verbose_eval=100\n        )\n\n    p = model.predict(X_valid, num_iteration=model.best_iteration)\n    auc = roc_auc_score(y_valid, p)\n\n    print('{} AUC: {}'.format(cnt, auc))\n\n    p = model.predict(X_test_stack, num_iteration=model.best_iteration)\n    if len(p_buf) == 0:\n        p_buf = np.array(p)\n    else:\n        p_buf += np.array(p)\n    auc_buf.append(auc)\n\n    cnt += 1\n    \nauc_mean = np.mean(auc_buf)\nauc_std = np.std(auc_buf)\nprint('AUC = {:.6f} +/- {:.6f}'.format(auc_mean, auc_std))\n\nlgb_preds = p_buf/cnt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Building model using Light GBM and finding AUC(Area Under Curve)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "9f5f9af16834bc014c31c402603a37615661a464"
      },
      "cell_type": "code",
      "source": "l_preds = pd.DataFrame(lgb_preds)\nl_preds.columns = ['project_is_approved']\nl_preds.head()\n\nsubmid = sub['id']\nlsub = pd.concat([submid, l_preds], axis=1)\nlsub.to_csv('lgbm_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1a69cc8843833665746b901f4d20176a466d86a9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "00b25a2d6ae79a4340ce1c8e13dcaf4a9b0e11f5"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}