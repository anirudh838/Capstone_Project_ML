{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/train.csv\", dtype={\"project_essay_3\": object, \"project_essay_4\": object})\ntarget = train['project_is_approved']\ntrain = train.drop('project_is_approved', axis=1)\n\ntest = pd.read_csv(\"../input/test.csv\", dtype={\"project_essay_3\": object, \"project_essay_4\": object})\nresources = pd.read_csv(\"../input/resources.csv\")\n\ntrain.fillna(('unk'), inplace=True)\ntest.fillna(('unk'), inplace=True)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02e17e69f8d6d54397ec86a082c261726cfa4ca5"
      },
      "cell_type": "code",
      "source": "from sklearn import preprocessing\nfrom tqdm import tqdm\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\n\nfeatures = [\n    'teacher_id', \n    'teacher_prefix', \n    'school_state', \n    'project_grade_category',\n    'project_subject_categories', \n    'project_subject_subcategories']\n\ndf_all = pd.concat([train, test], axis=0)\n    \nfor c in tqdm(features):\n    le = LabelEncoder()\n    le.fit(df_all[c].astype(str))\n    train[c] = le.transform(train[c].astype(str))\n    test[c] = le.transform(test[c].astype(str))",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|██████████| 6/6 [00:01<00:00,  3.62it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "079ecc5342ba23c5cf2d069a44cfc5ffe88dd0f0"
      },
      "cell_type": "code",
      "source": "# Feature engineering\n\n# Date and time\ntrain['project_submitted_datetime'] = pd.to_datetime(train['project_submitted_datetime'])\ntest['project_submitted_datetime'] = pd.to_datetime(test['project_submitted_datetime'])\n\n# Date as int may contain some ordinal value\ntrain['datetime_int'] = train['project_submitted_datetime'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['datetime_int'] = test['project_submitted_datetime'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n\n# Date parts\ntrain[\"year\"] = train[\"project_submitted_datetime\"].dt.year\ntrain[\"month\"] = train[\"project_submitted_datetime\"].dt.month\n#train['weekday'] = train['project_submitted_datetime'].dt.weekday\ntrain[\"hour\"] = train[\"project_submitted_datetime\"].dt.hour\ntrain[\"month_Day\"] = train['project_submitted_datetime'].dt.day\n#train[\"year_Day\"] = train['project_submitted_datetime'].dt.dayofyear\ntrain['datetime_dow'] = train['project_submitted_datetime'].dt.dayofweek\ntrain = train.drop('project_submitted_datetime', axis=1)\n\n\n# ****** Test data *********\ntest[\"year\"] = test[\"project_submitted_datetime\"].dt.year\ntest[\"month\"] = test[\"project_submitted_datetime\"].dt.month\n#test['weekday'] = test['project_submitted_datetime'].dt.weekday\ntest[\"hour\"] = test[\"project_submitted_datetime\"].dt.hour\ntest[\"month_Day\"] = test['project_submitted_datetime'].dt.day\n#test[\"year_Day\"] = test['project_submitted_datetime'].dt.dayofyear\ntest['datetime_dow'] = test['project_submitted_datetime'].dt.dayofweek\ntest = test.drop('project_submitted_datetime', axis=1)\n\n# Essay length\ntrain['e1_length'] = train['project_essay_1'].apply(len)\ntest['e1_length'] = train['project_essay_1'].apply(len)\n\ntrain['e2_length'] = train['project_essay_2'].apply(len)\ntest['e2_length'] = train['project_essay_2'].apply(len)\n\n# Title length\ntrain['project_title_len'] = train['project_title'].apply(lambda x: len(str(x)))\ntest['project_title_len'] = test['project_title'].apply(lambda x: len(str(x)))\n\n# Project resource summary length\ntrain['project_resource_summary_len'] = train['project_resource_summary'].apply(lambda x: len(str(x)))\ntest['project_resource_summary_len'] = test['project_resource_summary'].apply(lambda x: len(str(x)))\n\n# Has more than 2 essays?\ntrain['has_gt2_essays'] = train['project_essay_3'].apply(lambda x: 0 if x == 'unk' else 1)\ntest['has_gt2_essays'] = test['project_essay_3'].apply(lambda x: 0 if x == 'unk' else 1)",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "3c55da781662d22baf278a4a927e3e39745d60b3"
      },
      "cell_type": "code",
      "source": "resources['resources_total'] = resources['quantity'] * resources['price']\n\ndfr = resources.groupby(['id'], as_index=False)[['resources_total']].sum()\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['resources_total']].mean()\ndfr = dfr.rename(columns={'resources_total':'resources_total_mean'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['quantity']].count()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_count'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\ndfr = resources.groupby(['id'], as_index=False)[['quantity']].sum()\ndfr = dfr.rename(columns={'quantity':'resources_quantity_sum'})\ntrain = pd.merge(train, dfr, how='left', on='id').fillna(-1)\ntest = pd.merge(test, dfr, how='left', on='id').fillna(-1)\n\n# We're done with IDs for now\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "782e5680d4e8101cb8b5f5434858356832141b0f"
      },
      "cell_type": "code",
      "source": "train['project_essay'] = train.apply(lambda row: ' '.join([\n    str(row['project_title']),\n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']),\n    str(row['project_essay_4']),\n    str(row['project_resource_summary'])]), axis=1)\ntest['project_essay'] = test.apply(lambda row: ' '.join([\n    str(row['project_title']),\n    str(row['project_essay_1']), \n    str(row['project_essay_2']), \n    str(row['project_essay_3']),\n    str(row['project_essay_4']),\n    str(row['project_resource_summary'])]), axis=1)\n\ntrain = train.drop([\n    'project_title',\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4',\n    'project_resource_summary'], axis=1)\ntest = test.drop([\n    'project_title',\n    'project_essay_1', \n    'project_essay_2', \n    'project_essay_3', \n    'project_essay_4',\n    'project_resource_summary'], axis=1)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6fcf8cb9617e3079fcb7199507f384c9977f9a9a"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import punkt\nimport re\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef prep_text(text):\n    q = \"[\\'\\’\\´\\ʻ]\"\n    text = text.strip().lower()\n    text = re.sub('\\W+',' ', text)\n    text = re.sub(r'(\\\")', ' ', text)\n    text = re.sub(r\"\\\\r|\\\\n\", \" \", text)\n    text = re.sub(re.compile(\"won%st\" % q), \"will not\", text)\n    text = re.sub(re.compile(\"can%st\" % q), \"can not\", text)\n    text = re.sub(re.compile(\"n%st\" % q), \" not\", text)\n    text = re.sub(re.compile(\"%sre\" % q), \" are\", text)\n    text = re.sub(re.compile(\"%ss\" % q), \" is\", text)\n    text = re.sub(re.compile(\"%sd\" % q), \" would\", text)\n    text = re.sub(re.compile(\"%sll\" % q), \" will\", text)\n    text = re.sub(re.compile(\"%st\" % q), \" not\", text)\n    text = re.sub(re.compile(\"%sve\" % q), \" have\", text)\n    text = re.sub(re.compile(\"%sm\" % q), \" am\", text)\n    text = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n    return text\n\ntrain['project_essay'] = train['project_essay'].apply(lambda x: prep_text(x))\ntest['project_essay'] = test['project_essay'].apply(lambda x: prep_text(x))",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "bae239b92e2f9e4cd14d9f73c9f3e91a800a9c4f"
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfv = TfidfVectorizer(norm='l2', min_df=0,  max_features=8000, \n            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n            ngram_range=(1,2), use_idf=True, smooth_idf=False, sublinear_tf=True,\n            stop_words = 'english')",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "918b2bc921619da063acf72ad430dc262088c2d1"
      },
      "cell_type": "code",
      "source": "train_text = train['project_essay'].apply(lambda x: ' '.join(x))\ntest_text = test['project_essay'].apply(lambda x: ' '.join(x))\n\n# Fitting tfidf on train + test might be leaky\ntfv.fit(list(train_text.values) + list(test_text.values))\ntrain_tfv = tfv.transform(train_text)\ntest_tfv = tfv.transform(test_text)",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bf7b016a2029312ddaee661dff75c04a81e656b"
      },
      "cell_type": "code",
      "source": "from scipy.sparse import hstack, csr_matrix\nfeat_train = train.drop('project_essay', axis=1)\nfeat_test = test.drop('project_essay', axis=1)\n\nfeat_train = csr_matrix(feat_train.values)\nfeat_test = csr_matrix(feat_test.values)\n\nX_train_stack = hstack([feat_train, train_tfv[0:feat_train.shape[0]]])\nX_test_stack = hstack([feat_test, test_tfv[0:feat_test.shape[0]]])\n\nprint('Train shape: ', X_train_stack.shape, '\\n\\nTest Shape: ', X_test_stack.shape)",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train shape:  (182080, 8022) \n\nTest Shape:  (78035, 8022)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aecace5c8aebf4c196cd6314c85c5e69b0528202"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nimport random\nimport xgboost as xgb\n\nprint(\"Building model using XGBoost and finding AUC(Area Under Curve)\")\n\nkf = KFold(n_splits = 5, random_state = 28, shuffle = True)\n\ncv_scores = []\nxgb_preds = []\n\nfor train_index, test_index in kf.split(X_train_stack):\n    \n    # Split out a validation set\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train_stack, target, test_size=0.20, random_state=random.seed(28))\n    \n    # params are tuned with kaggle kernels in mind\n    xgb_params = {'eta': 0.15, \n                  'max_depth': 7, \n                  'subsample': 0.80, \n                  'colsample_bytree': 0.80, \n                  'objective': 'binary:logistic', \n                  'eval_metric': 'auc', \n                  'seed': 28\n                 }\n    \n    d_train = xgb.DMatrix(X_train, y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(X_test_stack)\n    \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    model = xgb.train(xgb_params, d_train, 2000, watchlist, verbose_eval=50, early_stopping_rounds=30)\n    cv_scores.append(float(model.attributes()['best_score']))\n    xgb_pred = model.predict(d_test)\n    xgb_preds.append(list(xgb_pred))",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Building model using XGBoost and finding AUC(Area Under Curve)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "61a87bfd620bf34d177a50fe47c722ffc9b27266"
      },
      "cell_type": "code",
      "source": "print(cv_scores)\nprint(np.mean(cv_scores))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d6613a6c669fd2af764de93a1a29eb620b134948"
      },
      "cell_type": "code",
      "source": "x_preds=[]\nfor i in range(len(xgb_preds[0])):\n    sum=0\n    for j in range(K):\n        sum+=xgb_preds[j][i]\n    x_preds.append(sum / K)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "21ff87e928462cc1cbbfdd11cbcbee15ed6243f9"
      },
      "cell_type": "code",
      "source": "# Peek at predictions\nx_preds[0:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ef9c23627bcbf06a5aa0ff8731bc2ff95e34c691"
      },
      "cell_type": "code",
      "source": "# XGB preds\nx_preds = pd.DataFrame(x_preds)\nx_preds.columns = ['project_is_approved']\n\nsubmid = sub['id']\nxsub = pd.concat([submid, x_preds], axis=1)\nxsub.to_csv('xgb_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "fd41045b6c17377ff39ad57503f0a87a799feca7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}